{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4217783",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Load Needed Packages](#1)<br>\n",
    "2. [Get Data](#2)<br>\n",
    "    2.1 [Read in the Data from csv](#2.1)<br>\n",
    "    2.2 [Clean Data](#2.2)<br>\n",
    "3. [Visualize Data](#3)<br>\n",
    "4. [Basket Analysis](#4)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47adb89",
   "metadata": {},
   "source": [
    "# Import necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6061123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a71b9fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3, MobileNet, Xception\n",
    "from tensorflow.keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648e48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e1a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "inceptionv3 = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "mobilenet = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "xception = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add a new classification layer\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "vgg16_output = vgg16.layers[-1].output\n",
    "flatten = Flatten()(vgg16_output)\n",
    "fc1 = Dense(256, activation='relu')(flatten)\n",
    "fc2 = Dense(1, activation='sigmoid')(fc1)\n",
    "vgg16_model = Model(inputs=vgg16.input, outputs=fc2)\n",
    "\n",
    "for layer in resnet50.layers:\n",
    "    layer.trainable = False\n",
    "resnet50_output = resnet50.layers[-1].output\n",
    "flatten = Flatten()(resnet50_output)\n",
    "fc1 = Dense(256, activation='relu')(flatten)\n",
    "fc2 = Dense(1, activation='sigmoid')(fc1)\n",
    "resnet50_model = Model(inputs=resnet50.input, outputs=fc2)\n",
    "\n",
    "for layer in inceptionv3.layers:\n",
    "    layer.trainable = False\n",
    "inceptionv3_output = inceptionv3.layers[-1].output\n",
    "flatten = Flatten()(inceptionv3_output)\n",
    "fc1 = Dense(256, activation='relu')(flatten)\n",
    "fc2 = Dense(1, activation='sigmoid')(fc1)\n",
    "inceptionv3_model = Model(inputs=inceptionv3.input, outputs=fc2)\n",
    "\n",
    "for layer in mobilenet.layers:\n",
    "    layer.trainable = False\n",
    "mobilenet_output = mobilenet.layers[-1].output\n",
    "flatten = Flatten()(mobilenet_output)\n",
    "fc1 = Dense(256, activation='relu')(flatten)\n",
    "fc2 = Dense(1, activation='sigmoid')(fc1)\n",
    "mobilenet_model = Model(inputs=mobilenet.input, outputs=fc2)\n",
    "\n",
    "for layer in xception.layers:\n",
    "    layer.trainable = False\n",
    "xception_output = xception.layers[-1].output\n",
    "flatten = Flatten()(xception_output)\n",
    "fc1 = Dense(256, activation='relu')(flatten)\n",
    "fc2 = Dense(1, activation='sigmoid')(fc1)\n",
    "xception_model = Model(inputs=xception.input, outputs=fc2)\n",
    "\n",
    "# Compile the models\n",
    "vgg16_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "resnet50_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "inception\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9dc5b",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05ca991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\jiali\\tensorflow_datasets\\curated_breast_imaging_ddsm\\patches\\3.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221e8a9dd1354065958ba15ed3f883c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f391fa8b0a245be915bf3a0818f716e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aced556f3f3412083286f87a67fbf9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Manual directory C:\\Users\\jiali\\tensorflow_datasets\\downloads\\manual does not exist or is empty. Create it and download/extract dataset artifacts in there using instructions:\nYou can download the images from\nhttps://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM\n\nBecause special software and libraries are needed to download and read the\nimages contained in the dataset, TFDS assumes that the user has downloaded the\noriginal DCIM files and converted them to PNG.\n\nThe following commands (or equivalent) should be used to generate the PNG\nfiles, in order to guarantee reproducible results:\n\n```sh\nfind $DATASET_DCIM_DIR -name '*.dcm' | \\\nxargs -n1 -P8 -I{} bash -c 'f={}; dcmj2pnm $f | convert - ${f/.dcm/.png}'\n```\n\nResulting images should be put in `manual_dir`, like:\n`<manual_dir>/Mass-Training_P_01981_RIGHT_MLO_1/1.3.6.../000000.png`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10072\\3446492888.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'curated_breast_imaging_ddsm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'validation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m       \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    615\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m     \u001b[0mdbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m       \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    638\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_from_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m           self._download_and_prepare(\n\u001b[0m\u001b[0;32m    641\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m               \u001b[0mdownload_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1446\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m         \u001b[0moptional_pipeline_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1448\u001b[1;33m       split_generators = self._split_generators(  # pylint: disable=unexpected-keyword-arg\n\u001b[0m\u001b[0;32m   1449\u001b[0m           \u001b[0mdl_manager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptional_pipeline_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m       )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\cbis_ddsm.py\u001b[0m in \u001b[0;36m_split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    280\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split_generators_original\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'patches'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split_generators_patches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m       raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\cbis_ddsm.py\u001b[0m in \u001b[0;36m_split_generators_patches\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    333\u001b[0m     }\n\u001b[0;32m    334\u001b[0m     \u001b[0mresource_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresources_urls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m     \u001b[0mpatients_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_csv_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;31m# Statistics about the resulting splits.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0mcached\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m       \u001b[0mcached\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pytype: disable=attribute-error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m       \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36mmanual_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    701\u001b[0m       )\n\u001b[0;32m    702\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_manual_dir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_manual_dir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m       raise AssertionError(\n\u001b[0m\u001b[0;32m    704\u001b[0m           \u001b[1;34mf'Manual directory {self._manual_dir} does not exist or is empty. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m           \u001b[1;34m'Create it and download/extract dataset artifacts in there using '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Manual directory C:\\Users\\jiali\\tensorflow_datasets\\downloads\\manual does not exist or is empty. Create it and download/extract dataset artifacts in there using instructions:\nYou can download the images from\nhttps://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM\n\nBecause special software and libraries are needed to download and read the\nimages contained in the dataset, TFDS assumes that the user has downloaded the\noriginal DCIM files and converted them to PNG.\n\nThe following commands (or equivalent) should be used to generate the PNG\nfiles, in order to guarantee reproducible results:\n\n```sh\nfind $DATASET_DCIM_DIR -name '*.dcm' | \\\nxargs -n1 -P8 -I{} bash -c 'f={}; dcmj2pnm $f | convert - ${f/.dcm/.png}'\n```\n\nResulting images should be put in `manual_dir`, like:\n`<manual_dir>/Mass-Training_P_01981_RIGHT_MLO_1/1.3.6.../000000.png`."
     ]
    }
   ],
   "source": [
    "dataset, info = tfds.load('curated_breast_imaging_ddsm', as_supervised=True, with_info=True)\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f94fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
